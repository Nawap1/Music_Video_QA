{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74aa6b2a",
   "metadata": {
    "_cell_guid": "39d0c7ce-b939-4a8c-9036-dbdb62e490fa",
    "_uuid": "5c6e996e-31d9-4e67-ae32-35044f42806b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-26T20:25:14.618071Z",
     "iopub.status.busy": "2025-01-26T20:25:14.617713Z",
     "iopub.status.idle": "2025-01-26T20:25:14.622490Z",
     "shell.execute_reply": "2025-01-26T20:25:14.621704Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009782,
     "end_time": "2025-01-26T20:25:14.623753",
     "exception": false,
     "start_time": "2025-01-26T20:25:14.613971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qwen_model = '/kaggle/input/qwen7b/qwen_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1482fe",
   "metadata": {
    "_cell_guid": "2a955d5f-ef6e-4545-8dce-ce397a33c65e",
    "_uuid": "691baa77-c546-46fa-b424-4e9e0deca29c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-26T20:25:14.629665Z",
     "iopub.status.busy": "2025-01-26T20:25:14.629435Z",
     "iopub.status.idle": "2025-01-26T20:25:37.741706Z",
     "shell.execute_reply": "2025-01-26T20:25:37.740588Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 23.116977,
     "end_time": "2025-01-26T20:25:37.743562",
     "exception": false,
     "start_time": "2025-01-26T20:25:14.626585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab8e411",
   "metadata": {
    "_cell_guid": "fe356e95-0552-4a0d-baf9-7f15e848f224",
    "_uuid": "67110ffb-d5ed-4bae-a1f8-baf3bef32aec",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-26T20:25:37.757111Z",
     "iopub.status.busy": "2025-01-26T20:25:37.756812Z",
     "iopub.status.idle": "2025-01-26T20:26:38.586796Z",
     "shell.execute_reply": "2025-01-26T20:26:38.585891Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 60.838462,
     "end_time": "2025-01-26T20:26:38.588627",
     "exception": false,
     "start_time": "2025-01-26T20:25:37.750165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.1.6: Fast Qwen2_Vl vision patching. Transformers: 4.48.1.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13823cfda9c47d59c09a70e1ff63637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299d048791ef42ffbb035b8e63c09f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2VLForConditionalGeneration(\n",
      "      (visual): Qwen2VisionTransformerPretrainedModel(\n",
      "        (patch_embed): PatchEmbed(\n",
      "          (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "        )\n",
      "        (rotary_pos_emb): VisionRotaryEmbedding()\n",
      "        (blocks): ModuleList(\n",
      "          (0-12): 13 x Qwen2VLVisionBlock(\n",
      "            (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): VisionSdpaAttention(\n",
      "              (qkv): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3840, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): VisionMlp(\n",
      "              (fc1): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): QuickGELUActivation()\n",
      "              (fc2): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (13-30): 18 x Qwen2VLVisionBlock(\n",
      "            (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): VisionSdpaAttention(\n",
      "              (qkv): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3840, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): VisionMlp(\n",
      "              (fc1): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): QuickGELUActivation()\n",
      "              (fc2): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (31): Qwen2VLVisionBlock(\n",
      "            (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): VisionSdpaAttention(\n",
      "              (qkv): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3840, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): VisionMlp(\n",
      "              (fc1): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1280, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): QuickGELUActivation()\n",
      "              (fc2): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1280, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (merger): PatchMerger(\n",
      "          (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "            (1): GELU(approximate='none')\n",
      "            (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (model): Qwen2VLModel(\n",
      "        (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x Qwen2VLDecoderLayer(\n",
      "            (self_attn): Qwen2VLSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): Qwen2VLRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        (rotary_emb): Qwen2VLRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import os\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = \"/kaggle/input/qwen7b/qwen_model/\"\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Verify the model\n",
    "print(type(model))  # Should output <class 'unsloth.FastVisionModel'>\n",
    "print(model)  # Inspect the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83743d0c",
   "metadata": {
    "_cell_guid": "f6e3fdf6-901d-4604-8cd2-1a51f5cc8733",
    "_uuid": "19db13c8-8411-4af4-960d-497b136a2210",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-26T20:26:38.603925Z",
     "iopub.status.busy": "2025-01-26T20:26:38.603662Z",
     "iopub.status.idle": "2025-01-26T20:27:18.799921Z",
     "shell.execute_reply": "2025-01-26T20:27:18.799145Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 40.205113,
     "end_time": "2025-01-26T20:27:18.801354",
     "exception": false,
     "start_time": "2025-01-26T20:26:38.596241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.generation.streamers.TextStreamer object at 0x7acd6b2b2020>\n",
      "The dance being performed in the video is the Bhairab Dance, which is a traditional Newari dance from Nepal.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Enable the model for inference\n",
    "model = FastVisionModel.for_inference(model)\n",
    "\n",
    "# Load the image as a PIL image\n",
    "image_path = \"/kaggle/input/videoframedata/frame_data/frames/BHAIRAB DANCE IN LONDON--IamAbatar--UCfj7fHQkQ5X5ogj8hpXwmJg---4Bkky5pgJg_segment1_clip_2/frame_3.jpg\"\n",
    "image = Image.open(image_path).resize((224, 224))  # Resize the image to 224x224\n",
    "\n",
    "# Prepare instruction and messages\n",
    "instruction = \"\"\"Answer the question based on the content of this image..\"\"\"\n",
    "question = \"Which dance is being performed in the video?\"\n",
    "\n",
    "# Prepare the messages in the required format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\", \"image\": image},  # Pass the resized PIL image directly\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply the chat template and tokenize the input\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# Define the text streamer\n",
    "text_streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True  # Skip showing the input prompt in the output\n",
    ")\n",
    "print(text_streamer)\n",
    "\n",
    "# Generate the output\n",
    "with torch.no_grad():  # Disable gradient computation during inference\n",
    "    _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128,\n",
    "                       use_cache=True, temperature=1.5, min_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70c0973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:27:18.818030Z",
     "iopub.status.busy": "2025-01-26T20:27:18.817747Z",
     "iopub.status.idle": "2025-01-26T20:27:23.712499Z",
     "shell.execute_reply": "2025-01-26T20:27:23.711748Z"
    },
    "papermill": {
     "duration": 4.904558,
     "end_time": "2025-01-26T20:27:23.713899",
     "exception": false,
     "start_time": "2025-01-26T20:27:18.809341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.generation.streamers.TextStreamer object at 0x7acd687a8a90>\n",
      "The vibe of the video is vibrant and celebratory, reflecting the joyous spirit of the Bhairab dance, which is performed during the Bhairab Jatra festival.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Load the image as a PIL image\n",
    "image_path = \"/kaggle/input/videoframedata/frame_data/frames/Kumari Dance_Charya Nritya_______ _____ 2076_Aloyana dongol_UCB7v02qvayFmzxiddLufw6w_trn6IKdHA_E_segment1_clip_2/frame_3.jpg\"\n",
    "image = Image.open(image_path).resize((224, 224))  # Resize the image to 224x224\n",
    "\n",
    "# Prepare instruction and messages\n",
    "instruction = \"\"\"\n",
    "Answer the question based on the content of this image. \n",
    "\"\"\"\n",
    "question = \"What is the vibe of the video?\"\n",
    "\n",
    "# Prepare the messages in the required format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\", \"image\": image},  # Pass the resized PIL image directly\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply the chat template and tokenize the input\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# Define the text streamer\n",
    "text_streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True  # Skip showing the input prompt in the output\n",
    ")\n",
    "print(text_streamer)\n",
    "\n",
    "# Generate the output\n",
    "with torch.no_grad():  # Disable gradient computation during inference\n",
    "    _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128,\n",
    "                       use_cache=True, temperature=1.5, min_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd082ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:27:23.732647Z",
     "iopub.status.busy": "2025-01-26T20:27:23.732341Z",
     "iopub.status.idle": "2025-01-26T20:27:27.562006Z",
     "shell.execute_reply": "2025-01-26T20:27:27.561171Z"
    },
    "papermill": {
     "duration": 3.840447,
     "end_time": "2025-01-26T20:27:27.563435",
     "exception": false,
     "start_time": "2025-01-26T20:27:23.722988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.generation.streamers.TextStreamer object at 0x7acd6b5776d0>\n",
      "The video features two dancers, a man and a woman, performing the traditional Tibetan dance known as the \"Chyabrung.\"<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Load the image as a PIL image\n",
    "image_path = '/kaggle/input/videoframedata/frame_data/frames/-DHIME DANCE AND MUSIC _ Culture Of Newar--Sam Audio Video--S4PcERz4Eio_segment3_clip_1/frame_2.jpg'\n",
    "image = Image.open(image_path).resize((224, 224))  # Resize the image to 224x224\n",
    "\n",
    "# Prepare instruction and messages\n",
    "instruction = \"\"\"\n",
    "Answer the question based on the content of this image. \n",
    "\n",
    "\"\"\"\n",
    "question = \"How many dancers are in the video?\"\n",
    "\n",
    "# Prepare the messages in the required format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\", \"image\": image},  # Pass the resized PIL image directly\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply the chat template and tokenize the input\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# Define the text streamer\n",
    "text_streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True  # Skip showing the input prompt in the output\n",
    ")\n",
    "print(text_streamer)\n",
    "\n",
    "# Generate the output\n",
    "with torch.no_grad():  # Disable gradient computation during inference\n",
    "    _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128,\n",
    "                       use_cache=True, temperature=1.5, min_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c38c7fd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:27:27.583982Z",
     "iopub.status.busy": "2025-01-26T20:27:27.583727Z",
     "iopub.status.idle": "2025-01-26T20:27:32.258355Z",
     "shell.execute_reply": "2025-01-26T20:27:32.257460Z"
    },
    "papermill": {
     "duration": 4.686059,
     "end_time": "2025-01-26T20:27:32.259701",
     "exception": false,
     "start_time": "2025-01-26T20:27:27.573642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.generation.streamers.TextStreamer object at 0x7acd5c532050>\n",
      "The instruments being played in the video include traditional Nepalese drums, known as \"dhol,\" which are played in a rhythmic pattern to accompany the dance.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Load the image as a PIL image\n",
    "image_path = '/kaggle/input/videoframedata/frame_data/frames/-_ Newari Dhime baja dance performance at Nyatapola Temple_ Bhaktapur _ Yatra Nepal _ 2080_12_26 _--YATRA NEPAL--vjvXAPeAoYc_segment2_clip_1/frame_4.jpg'\n",
    "image = Image.open(image_path).resize((224, 224))  # Resize the image to 224x224\n",
    "\n",
    "# Prepare instruction and messages\n",
    "instruction = \"\"\"\n",
    "Answer the question based on the content of this image. \n",
    "\n",
    "\"\"\"\n",
    "question = \"What kind of instruments are being used in this video?\"\n",
    "\n",
    "# Prepare the messages in the required format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\", \"image\": image},  # Pass the resized PIL image directly\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply the chat template and tokenize the input\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# Define the text streamer\n",
    "text_streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True  # Skip showing the input prompt in the output\n",
    ")\n",
    "print(text_streamer)\n",
    "\n",
    "# Generate the output\n",
    "with torch.no_grad():  # Disable gradient computation during inference\n",
    "    _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128,\n",
    "                       use_cache=True, temperature=1.5, min_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce71d5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:27:32.282058Z",
     "iopub.status.busy": "2025-01-26T20:27:32.281812Z",
     "iopub.status.idle": "2025-01-26T20:27:38.568341Z",
     "shell.execute_reply": "2025-01-26T20:27:38.567628Z"
    },
    "papermill": {
     "duration": 6.299143,
     "end_time": "2025-01-26T20:27:38.569687",
     "exception": false,
     "start_time": "2025-01-26T20:27:32.270544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.generation.streamers.TextStreamer object at 0x7acd5cccd150>\n",
      "The setting of this video is the Durbar Square in Kathmandu, Nepal, featuring the vibrant Newari culture and traditional Newari dances, specifically the Bhairab dance, which is performed during the Indra Jatra festival.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Load the image as a PIL image\n",
    "image_path = '/kaggle/input/videoframedata/frame_data/frames/-_ Newari Dhime baja dance performance at Nyatapola Temple_ Bhaktapur _ Yatra Nepal _ 2080_12_26 _--YATRA NEPAL--vjvXAPeAoYc_segment2_clip_1/frame_4.jpg'\n",
    "image = Image.open(image_path).resize((224, 224))  # Resize the image to 224x224\n",
    "\n",
    "# Prepare instruction and messages\n",
    "instruction = \"\"\"\n",
    "Answer the question based on the content of this image. \n",
    "\n",
    "\"\"\"\n",
    "question = \"Where is the setting of this video?\"\n",
    "\n",
    "# Prepare the messages in the required format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\", \"image\": image},  # Pass the resized PIL image directly\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply the chat template and tokenize the input\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# Define the text streamer\n",
    "text_streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True  # Skip showing the input prompt in the output\n",
    ")\n",
    "print(text_streamer)\n",
    "\n",
    "# Generate the output\n",
    "with torch.no_grad():  # Disable gradient computation during inference\n",
    "    _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128,\n",
    "                       use_cache=True, temperature=1.5, min_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ccc1656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:27:38.594821Z",
     "iopub.status.busy": "2025-01-26T20:27:38.594548Z",
     "iopub.status.idle": "2025-01-26T20:27:48.756397Z",
     "shell.execute_reply": "2025-01-26T20:27:48.755340Z"
    },
    "papermill": {
     "duration": 10.176229,
     "end_time": "2025-01-26T20:27:48.758361",
     "exception": false,
     "start_time": "2025-01-26T20:27:38.582132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.generation.streamers.TextStreamer object at 0x7acd68665360>\n",
      "The masked deities in the video are likely representations of Bhairab, a powerful deity associated with the Hindu pantheon, often depicted with a fierce appearance to symbolize protection and destruction. These masks are part of the traditional Bhairab dance, which is performed during the Bhairab Jatra festival in Nepal, showcasing the deity's strength and the community's reverence for him.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Load the image as a PIL image\n",
    "image_path = '/kaggle/input/videoframedata/frame_data/frames/BHAIRAB DANCE IN LONDON--IamAbatar--UCfj7fHQkQ5X5ogj8hpXwmJg---4Bkky5pgJg_segment2_clip_2/frame_4.jpg'\n",
    "image = Image.open(image_path).resize((224, 224))  # Resize the image to 224x224\n",
    "\n",
    "# Prepare instruction and messages\n",
    "instruction = \"\"\"\n",
    "Answer the question based on the content of this image. \n",
    "\n",
    "\"\"\"\n",
    "question = \"Who are these masked deties in video?\"\n",
    "\n",
    "# Prepare the messages in the required format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\", \"image\": image},  # Pass the resized PIL image directly\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply the chat template and tokenize the input\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# Define the text streamer\n",
    "text_streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True  # Skip showing the input prompt in the output\n",
    ")\n",
    "print(text_streamer)\n",
    "\n",
    "# Generate the output\n",
    "with torch.no_grad():  # Disable gradient computation during inference\n",
    "    _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128,\n",
    "                       use_cache=True, temperature=1.5, min_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de289886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:27:48.792362Z",
     "iopub.status.busy": "2025-01-26T20:27:48.792019Z",
     "iopub.status.idle": "2025-01-26T20:27:53.763639Z",
     "shell.execute_reply": "2025-01-26T20:27:53.762807Z"
    },
    "papermill": {
     "duration": 4.988598,
     "end_time": "2025-01-26T20:27:53.764959",
     "exception": false,
     "start_time": "2025-01-26T20:27:48.776361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.generation.streamers.TextStreamer object at 0x7acd5c39bf10>\n",
      "The video features two masked deities, representing the Bhairab and Bhairabini, who are central figures in the Newari culture, symbolizing power and protection.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Load the image as a PIL image\n",
    "image_path = '/kaggle/input/videoframedata/frame_data/frames/Indra Jatra 2074 _Halchok Bhairav Dance_--Sujit Sind--UC7ObPu4bVxKbJxBdFsnzqSg--7SI3qNlocBw_cut_clip_2/frame_4.jpg'\n",
    "image = Image.open(image_path).resize((224, 224))  # Resize the image to 224x224\n",
    "\n",
    "# Prepare instruction and messages\n",
    "instruction = \"\"\"\n",
    "Answer the question based on the content of this image. \n",
    "\n",
    "\"\"\"\n",
    "question = \"How many masked deties are in the video and what do they represnet?\"\n",
    "\n",
    "# Prepare the messages in the required format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\", \"image\": image},  # Pass the resized PIL image directly\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply the chat template and tokenize the input\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# Define the text streamer\n",
    "text_streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True  # Skip showing the input prompt in the output\n",
    ")\n",
    "print(text_streamer)\n",
    "\n",
    "# Generate the output\n",
    "with torch.no_grad():  # Disable gradient computation during inference\n",
    "    _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128,\n",
    "                       use_cache=True, temperature=1.5, min_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e986cf15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:27:53.798271Z",
     "iopub.status.busy": "2025-01-26T20:27:53.797986Z",
     "iopub.status.idle": "2025-01-26T20:27:57.593793Z",
     "shell.execute_reply": "2025-01-26T20:27:57.592872Z"
    },
    "papermill": {
     "duration": 3.814086,
     "end_time": "2025-01-26T20:27:57.595215",
     "exception": false,
     "start_time": "2025-01-26T20:27:53.781129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.generation.streamers.TextStreamer object at 0x7acd5caeae00>\n",
      "Yes, the Kumari dance is being performed in the video, showcasing the traditional attire and elaborate costumes of the Kumari dancers.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Load the image as a PIL image\n",
    "image_path = '/kaggle/input/videoframedata/frame_data/frames/KUMARI_TheJoshiramesh_UCOg58aroiIixHGkavfujkzA_Gg_9ly6zNOc_segment2_clip_2/frame_2.jpg'\n",
    "image = Image.open(image_path).resize((224, 224))  # Resize the image to 224x224\n",
    "\n",
    "# Prepare instruction and messages\n",
    "instruction = \"\"\"\n",
    "Answer the question based on the content of this image. \n",
    "\n",
    "\"\"\"\n",
    "question = \"Is Kumari dance being performed on the video?\"\n",
    "\n",
    "# Prepare the messages in the required format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\", \"image\": image},  # Pass the resized PIL image directly\n",
    "            {\"type\": \"text\", \"text\": question}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply the chat template and tokenize the input\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# Define the text streamer\n",
    "text_streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True  # Skip showing the input prompt in the output\n",
    ")\n",
    "print(text_streamer)\n",
    "\n",
    "# Generate the output\n",
    "with torch.no_grad():  # Disable gradient computation during inference\n",
    "    _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128,\n",
    "                       use_cache=True, temperature=1.5, min_p=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fc284",
   "metadata": {
    "papermill": {
     "duration": 0.016043,
     "end_time": "2025-01-26T20:27:57.628127",
     "exception": false,
     "start_time": "2025-01-26T20:27:57.612084",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6544541,
     "sourceId": 10576580,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 219190345,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 168.808914,
   "end_time": "2025-01-26T20:28:00.858102",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-26T20:25:12.049188",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00ecb60f82a7474f882c9a70929de8ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b5549157fdc4d2399a054375341257b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0f880f6b5903480393ed3a8a29881c5c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "191ac98978a349af9accb2a9da6872e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1b285f5ab37447709ace6305f8f80bec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1dd3d40b228a4c399cb25817e00264e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "24d58e7dfd714dbf86d6c5c7dab84a1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b1be9dc8e7a74abd844691ea4f4e2b8b",
       "placeholder": "​",
       "style": "IPY_MODEL_191ac98978a349af9accb2a9da6872e7",
       "tabbable": null,
       "tooltip": null,
       "value": " 238/238 [00:00&lt;00:00, 17.8kB/s]"
      }
     },
     "299d048791ef42ffbb035b8e63c09f97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bf3a8004d1754bc9a9281efb071a1504",
        "IPY_MODEL_80a839a4a41241a7a5a3f0cabb8a4699",
        "IPY_MODEL_24d58e7dfd714dbf86d6c5c7dab84a1f"
       ],
       "layout": "IPY_MODEL_758c2258f1c04b5f9956db557057b60f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4f2ba5a4ecff4e598472a664103e9ccf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1dd3d40b228a4c399cb25817e00264e0",
       "max": 6356703012.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1b285f5ab37447709ace6305f8f80bec",
       "tabbable": null,
       "tooltip": null,
       "value": 6356702406.0
      }
     },
     "758c2258f1c04b5f9956db557057b60f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80a839a4a41241a7a5a3f0cabb8a4699": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9cf61234937c4bf9875baeed2243c90b",
       "max": 238.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b6f4cc5bf0d04289be3671fb1458a813",
       "tabbable": null,
       "tooltip": null,
       "value": 238.0
      }
     },
     "9cf61234937c4bf9875baeed2243c90b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1be9dc8e7a74abd844691ea4f4e2b8b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4aff491420a4b4cbd811aa5afac61e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f880f6b5903480393ed3a8a29881c5c",
       "placeholder": "​",
       "style": "IPY_MODEL_f15d963ba8c84da688e8ad73a1f071f9",
       "tabbable": null,
       "tooltip": null,
       "value": " 6.36G/6.36G [00:12&lt;00:00, 1.22GB/s]"
      }
     },
     "b6f4cc5bf0d04289be3671fb1458a813": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bf3a8004d1754bc9a9281efb071a1504": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_00ecb60f82a7474f882c9a70929de8ab",
       "placeholder": "​",
       "style": "IPY_MODEL_0b5549157fdc4d2399a054375341257b",
       "tabbable": null,
       "tooltip": null,
       "value": "generation_config.json: 100%"
      }
     },
     "c30c308dfb414035ac1c07e407d1ba82": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb9cecdcc9a04622929bf93262b6f3e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c30c308dfb414035ac1c07e407d1ba82",
       "placeholder": "​",
       "style": "IPY_MODEL_cfc1ea9aa953469b818ad140eea2aa10",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "cfc1ea9aa953469b818ad140eea2aa10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e13823cfda9c47d59c09a70e1ff63637": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cb9cecdcc9a04622929bf93262b6f3e9",
        "IPY_MODEL_4f2ba5a4ecff4e598472a664103e9ccf",
        "IPY_MODEL_b4aff491420a4b4cbd811aa5afac61e2"
       ],
       "layout": "IPY_MODEL_f97f24e6c97946a3a07752de8bd453b6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f15d963ba8c84da688e8ad73a1f071f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f97f24e6c97946a3a07752de8bd453b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
